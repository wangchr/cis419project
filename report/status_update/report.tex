%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% Example Paper for Penn Machine Learning - Based on ICML 2014 template %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% If you rely on Latex2e packages, like most modern people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{pennML2015} 


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Del Vecchio, Kibria, Wang}

\begin{document} 

\twocolumn[
\icmltitle{eMeriL: A Recipe Rating Prediction System Based on Foodpairing}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.
\icmlauthor{Philip Del Vecchio}{delphi@seas.upenn.edu}
\icmlauthor{Golam Kibria}{gkibria@seas.upenn.edu}
\icmlauthor{Christian Wang}{wangchr@seas.upenn.edu}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{foodpairing recipe network}

\vskip 0.3in
]

\begin{abstract} 
We propose a new recipe rating prediction system that exploits the foodpairing theory, which claims that ingredients taste better when paired with certain other ingredients. By using this approach, we examine the presence of ingredients as well as the pairwise combinations of ingredients. In addition, the system includes an ingredient flavor network, which relates ingredients based on the number of chemical flavor compounds they share. This network quantitatively characterizes the foodpairing theory and allows us to incorporate it as features.
\end{abstract} 

\section{Introduction}
Chefs are always looking for new and exciting ways to combine and present ingredients in the dishes they serve. Traditionally, this process has either involved gathering empirical evidence in the kitchen or using historical data found in cookbooks and recorded recipes. However, these methods are inefficient because of the relative amount of time needed to parse through large amounts of recipe data and because creativity is constrained by convention and legacy. In this pursuit of new food combinations, many researchers have systematically examined the way ingredients are used together. Much work has been done on characterizing the chemical flavor and aromatic compounds that ingredients share and how we have naturally used this sensory information to pair foods \cite{leffingwell00,burdock09}. The examination of food at the molecular level has given rise to the foodpairing theory, which hypothesizes that certain foods taste better when matched with other foods because of shared flavor compounds. The foodpairing theory has been popularized by several high-rank restaurants as an approach to discovering unconventional food combinations such as caviar and chocolate \cite{blumenthal02}.

We propose a new method for predicting the rating of a recipe based on an ingredient-flavor network, which describes the sharing of flavor compounds between ingredients, as well as existing recipes. A systematic and data-driven approach to the cooking creativity process could potentially allow chefs to quickly filter out poor flavor combinations and accelerate the rate at which innovative yet delicious dishes are conceived.

\section{Related Work}
Understanding systematically how cooking works has been an application of interest in machine learning and data science \cite{harvey13,jagithyala14,liu14,yu13,teng12}. Researchers have derived features from a variety of sources.

Because of the large number of users available in an online recipe-sharing community, user reviews have been a source of features that potentially provide useful insight into the quality of a recipe. Liu et al. leverages recipe reviews of users of online recipe database websites, framing the learning problem using a linguistic approach \yrcite{liu14}. 

Personalized recipe recommendation systems have also been explored, particularly when based on nutritional value. These systems aim to encourage healthy living and eating by suggesting recipes match users' nutritional needs as well as ingredient combination preferences. \cite{harvey13} investigated incorporating nutritional content of ingredients along with preference for ingredients and ingredient combinations, concluding that including nutritional information was beneficial to their results, though insignificant due to a simple incorporation of those features. The idea of personalized recipe recommendations has also been expanded to group-based recipe recommendations by creating models that accurately recommend recipes to families of users \cite{berkovsky10}.

Methods of food preparation, such as frying or grilling, also intuitively alter the taste of foods and consequently the quality of recipes. Yu et al. proposed a simple linguistic approach to extract cooking methods and the complexity of the recipe as features used in recipe recommendation \yrcite{yu13}.

Network-based approaches to understand the relationships and combinations of ingredients have also been developed. Teng et al. examines reviews to determine substitutes for certain ingredients or modifications to the recipes. These were used to create complement and substitute ingredient networks from which features were extracted for recipe recommendation \yrcite{teng12}. Kinouchi et al. examined the ingredient usage in recipes recorded in a variety of cookbooks. By counting the frequency of ingredients, they created a network deriving the relationship between ingredients and recipes \yrcite{kinouchi08}.

There has also been interest in the foodpairing approach to examine the ingredient usage methodology in cooking through understanding chemical composition of foods. The chemical compounds of foods have been recorded in sources such as \textit{Fenaroli's Handbook of Flavor Ingredients} \cite{burdock09} and the Volatile Compounds in Food Database \cite{leffingwell00}. Ahn et al. used the data recorded in \textit{Fenaroli's Handbook} to create a weighted “flavor network” representing the sharing of chemical flavor compounds between foods \yrcite{ahn11}. This flavor network was used to empirically evaluate of the foodpairing theory in different cultures. We plan to use this flavor network to generate features in our own experiments.

\section{Method}
\subsection{Data Preprocessing}
We obtained our recipe data by scraping several popular recipe websites, including chowhound.com and allrecipes.com. Most recipe websites use a standardized schema to present their content, with particular labels on ingredients and quantities that eases scraping. With a python script using the Beautiful Soup library for HTML parsing and Pymongo for database interaction, we were able to scrape the names, ingredients, and categories for over 10,000 recipes and store them in a MongoDB. Ingredient quantities and preparation steps are notably absent, as they are beyond the scope of this study.

After obtaining the recipes, we vectorized the ingredients with ingredient-pair counts. Frequently paired ingredients are indicative of good flavor combinations. We plan to later incorporate our research on flavor-pairs to weight the ingredient pairs in favor of established, studied pairs.

Though we initially planned to classify the data based on the ratings from users of the websites, we found that most of the ratings were in the 4-5 range, and did not provide enough disparity to provide a meaningful split. To combat this, we generated recipes with an average number of ingredients chosen uniformly at random from the set of known ingredients to use as negative examples, and treated the real recipes as positive examples. A recipe with ten random ingredients is almost certainly not going to be good, and so random recipes serve well as negative examples.

\section{Experiments and Results}
With a 60/40 train/test split on the shuffled data, we trained an SVC with an RBF kernel, a random forest classifier, and an AdaBoost classifier. Using grid search to determine the best parameters for each of these classifiers, we determined that the AdaBoost classifier, with a 80.3\% accuracy, was the best classifier.

\section{Future Work}
We plan on gathering more data from other sources online. We also plan to do more in-depth preprocessing of the scraped text data to obtain better accuracy and feature representation. This will include cleaning of ingredient strings to ensure there are no unintended duplicates represented as different strings. We also will experiment with the inclusion of the ingredient flavor network as part of the feature set to see whether that gives better results. In addition, because we are currently using a binary classifer, we plan to find a way to extend the predictor to a multi-class or real-valued classifier so that the predictions are more meaningful.

\section*{Acknowledgments} 
None.

\bibliography{report.bib}
\bibliographystyle{icml2014}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
